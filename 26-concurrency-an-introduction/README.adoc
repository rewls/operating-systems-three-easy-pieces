= 26 Concurrency: An Introduction
:figure-caption: Figure 26.
:imagesdir: image
:toc: left

* Thus far, we have seen the development of the basic abstractions that the OS
  performs.
* We have seen how to take a single physical CPU and turn it into multiple
  *virtual CPUs*, thus enabling the illusion of multiple programs running at
  the same time.
* We have also seen how to create the illusion of a large, private *virtual
  memory* for each process; this abstraction of the *address space* enables each
  program to behave as if it has its own memory when indeed the OS is secretly
  multiplexing address spaces across physical memory (and sometimes, disk).

'''

* In this chapter, we introduce a new abstraction for a single running
  process: that of a *thread*.
* Instead of our classic view of a single point of execution within a program
  (i.e., a single PC where instructions are being fetched from and executed),
  a *multi-threaded* program has more than one point of execution (i.e.,
  multiple PCs, each of which is being fetched and executed from).
* Perhaps another way to think of this is that each thread is very much like a
  separate process, except for one difference: they _share_ the same address
  space and thus can access the same data.

'''

* The state of a single thread is thus very similar to that of a process.
* It has a program counter (PC) that tracks where the program is fetching
  instructions from.
* Each thread has its own private set of registers it uses for computation;
  thus, if there are two threads that are running on a single processor, when
  switching from running one (T1) to running the other (T2), a *context
  switch* must take place.
* The context switch between threads is quite similar to the context switch
  between processes, as the register state of T1 must be saved and the
  register state of T2 restored before running T2.
* With processes, we saved state to a *process control block (PCB)*; now,
  we'll need one or more *thread control blocks (TCBs)* to store the state of
  each thread of a process.
* There is one major difference, though, in the context switch we perform
  between threads as compared to processes: the address space remains the same
  (i.e., there is no need to switch which page table we are using).

'''

* One other major difference between threads and processes concerns the stack.
* In our simple model of the address space of a classic process (which we can
  now call a *single-threaded* process), there is a single stack, usually
  residing at the bottom of the address space (Figure 26.1, left).

'''

* However, in a multi-threaded process, each thread runs independently and of
  course may call into various routines to do whatever work it is doing.
* Instead of a single stack in the address space, there will be one per
  thread.
* Let's say we have a multi-threaded process that has two threads in it; the
  resulting address space looks different (Figure 26.1, right).

.Single-Threaded And Multi-Threaded Address Spaces
image::figure-26-01.png[]

* In this figure, you can see two stacks spread throughout the address space
  of the process.
* Thus, any stack-allocated variables, parameters, return values, and other
  things that we put on the stack will be placed in what is sometimes called
  *thread-local* storage, i.e., the stack of the relevant thread.

'''

* You might also notice how this ruins our beautiful address space layout.
* Before, the stack and heap could grow independently and trouble only arose
  when you ran out of room in the address space.
* Here, we no longer have such a nice situation.
* Fortunately, this is usually OK, as stacks do not generally have to be very
  large (the exception being in programs that make heavy use of recursion).

== 26.1 Why Use Threads?

* Before getting into the details of threads and some of the problems you
  might have in writing multi-threaded programs, let's first answer a more
  simple question.
* Why should you use threads at all?

'''

* As it turns out, there are at least two major reasons you should use
  threads.
* The first is simple: *parallelism*.
* Imagine you are writing a program that performs operations on very large
  arrays, for example, adding two large arrays together, or incrementing the
  value of each element in the array by some amount.
* If you are running on just a single processor, the task is straightforward:
  just perform each operation and be done.
* However, if you are executing the program on a system with multiple
  processors, you have the potential of speeding up this process considerably
  by using the processors to each perform a portion of the work.
* The task of transforming your standard *single-threaded* program into a
  program that does this sort of work on multiple CPUs is called
  *parallelization*, and using a thread per CPU to do this work is a natural
  and typical way to make programs run faster on modern hardware.

'''

* The second reason is a bit more subtle: to avoid blocking program progress
  due to slow I/O.
* Imagine that you are writing a program that performs different types of I/O:
  either waiting to send or receive a message, for an explicit disk I/O to
  complete, or even (implicitly) for a page fault to finish.
* Instead of waiting, your program may wish to do something else, including
  utilizing the CPU to perform computation, or even issuing further I/O
  requests.
* Using threads is a natural way to avoid getting stuck; while one thread in
  your program waits (i.e., is blocked waiting for I/O), the CPU scheduler can
  switch to other threads, which are ready to run and do something useful.
* Threading enables overlap of I/O with other activities within a single
  program, much like multiprogramming did for processes across programs; as a
  result, many modern server-based applications (web servers, database
  management systems, and the like) make use of threads in their
  implementations.

'''

* Of course, in either of the cases mentioned above, you could use multiple
  processes instead of threads.
* However, threads share an address space and thus make it easy to share data,
  and hence are a natural choice when constructing these types of programs.
* Processes are a more sound choice for logically separate tasks where little
  sharing of data structures in memory is needed.

== 26.2 An Example: Thread Creation

* Let's get into some of the details.
* Say we wanted to run a program that creates two threads, each of which does
  some independent work, in this case printing "A" or "B".
* The code is shown in Figure 26.2 (page 4).

:figure-number: 2
.{figure-caption} {figure-number}. Simple Thread Creation Code (`t0.c`)
[,c]
----
include::t0.c[]
----

* The main program creates two threads, each of which will run the function
  `mythread()`, though with different arguments (the string `A` or `B`).
* Once a thread is created, it may start running right away (depending on the
  whims of the scheduler); alternately, it may be put in a "ready" but not
  "running" state and thus not run yet.
* Of course, on a multiprocessor, the threads could even be running at the
  same time, but let's not worry about this possibility quite yet.

'''

* After creating the two threads (let's call them T1 and T2), the main thread
  calls `pthread_join()`, which waits for a particular thread to complete.
* It does so twice, thus ensuring T1 and T2 will run and complete before
  finally allowing the main thread to run again; when it does, it will print
  "main: end" and exit.
* Overall, three threads were employed during this run: the main thread, T1,
  and T2.

'''

* Let us examine the possible execution ordering of this little program.
* In the execution diagram (Figure 26.3, page 5), time increases in the
  downwards direction, and each column shows when a different thread (the main
  one, or Thread 1, or Thread 2) is running.

.Thread Trace (1)
image::figure-26-03.png[]

* Note, however, that this ordering is not the only possible ordering.
* In fact, given a sequence of instructions, there are quite a few, depending
  on which thread the scheduler decides to run at a given point.
* For example, once a thread is created, it may run immediately, which would
  lead to the execution shown in Figure 26.4 (page 5).

.Thread Trace (2)
image::figure-26-04.png[]

* We also could even see "B" printed before "A", if, say, the scheduler
  decided to run Thread 2 first even though Thread 1 was created earlier;
  there is no reason to assume that a thread that is created first will run
  first.
* Figure 26.5 (page 6) shows this final execution ordering, with Thread 2
  getting to strut its stuff before Thread 1.

.Thread Trace (3)
image::figure-26-05.png[]

* As you might be able to see, one way to think about thread creation is that
  it is a bit like making a function call; however, instead of first executing
  the function and then returning to the caller, the system instead creates a
  new thread of execution for the routine that is being called, and it runs
  independently of the caller, perhaps before returning from the create, but
  perhaps much later.
* What runs next is determined by the OS scheduler, and although the scheduler
  likely implements some sensible algorithm, it is hard to know what will run
  at any given moment in time.

'''

* As you also might be able to tell from this example, threads make life
  complicated: it is already hard to tell what will run when!
* Computers are hard enough to understand without concurrency.
* Unfortunately, with concurrency, it simply gets worse.
* Much worse.

== 26.3 Why It Gets Worse: Shared Data

* The simple thread example we showed above was useful in showing how threads
  are created and how they can run in different orders depending on how the
  scheduler decides to run them.
* What it doesn't show you, though, is how threads interact when they access
  shared data.

'''

* Let us imagine a simple example where two threads wish to update a global
  shared variable.
* The code we'll study is in Figure 26.6 (page 7).

:figure-number: 6
.{figure-caption} {figure-number}. Sharing Data: Uh Oh (`t1.c`)
[,c]
----
include::t1.c[]
----

* Here are a few notes about the code.
* First, as Stevens suggests [SR05], we wrap the thread creation and join
  routines to simply exit on failure; for a program as simple as this one, we
  want to at least notice an error occurred (if it did), but not do anything
  very smart about it (e.g., just exit).
* Thus, `Pthread_create()` simply calls `pthread_create()` and makes sure the
  return code is 0; if it isn't, `Pthread_create()` just prints a message and
  exits.

'''

* Second, instead of using two separate function bodies for the worker
  threads, we just use a single piece of code, and pass the thread an argument
  (in this case, a string) so we can have each thread print a different letter
  before its messages.

'''

* Finally, and most importantly, we can now look at what each worker is trying
  to do: add a number to the shared variable counter, and do so 10 million
  times (1e7) in a loop.
* Thus, the desired final result is: 20,000,000.

'''

* We now compile and run the program, to see how it behaves.
* Sometimes, everything works how we might expect:

....
prompt> gcc -o main main.c -Wall -pthread; ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 20000000)
....

* Unfortunately, when we run this code, even on a single processor, we don't
  necessarily get the desired result.
* Sometimes, we get:

....
prompt> ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 19345221)
....

* Let's try it one more time, just to see if we've gone crazy.
* After all, aren't computers supposed to produce *deterministic* results, as
  you have been taught?!
* Perhaps your professors have been lying to you? (gasp)

....
prompt> ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 19221041)
....

* Not only is each run wrong, but also yields a _different_ result!
* A big question remains: why does this happen?

.Tip: Know and use your tools
****
* You should always learn new tools that help you write, debug, and understand
  computer systems.
* Here, we use a neat tool called a *disassembler*.
* When you run a disassembler on an executable, it shows you what assembly
  instructions make up the program.
* For example, if we wish to understand the low-level code to update a counter
  (as in our example), we run `objdump` (Linux) to see the assembly code:
+
....
prompt> objdump -d main
....

* Doing so produces a long listing of all the instructions in the program,
  neatly labeled (particularly if you compiled with the `-g` flag), which
  includes symbol information in the program.
* The `objdump` program is just one of many tools you should learn how to use;
  a debugger like `gdb`, memory profilers like `valgrind` or purify, and of
  course the compiler itself are others that you should spend time to learn
  more about; the better you are at using your tools, the better systems
  you'll be able to build.
****

== References

[SR05] "Advanced Programming in the UNIX Environment" by W. Richard Stevens and Stephen A. Rago. Addison-Wesley, 2005.::
* As we've said many times, buy this book, and read it, in little chunks,
  preferably before going to bed.
* This way, you will actually fall asleep more quickly; more importantly, you
  learn a little more about how to become a serious UNIX programmer.
