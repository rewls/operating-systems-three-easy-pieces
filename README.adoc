= Operating Systems: Three Easy Pieces

[quote]
____
Operating Systems: Three Easy Pieces by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau (University of Wisconsin-Madison) (Version 1.10)

[%autowidth]
|===
|Publication date	|Print length

|September 1, 2018	|747 pages
|===
____
 
== Preface

=== To Everyone

* The book is called *Operating Systems: Three Easy Pieces* (available at
  `http://ostep.org`), and the title is obviously an homage to one of the
  greatest sets of lecture notes ever created, by one Richard Feynman on the
  topic of Physics [F96].
* While this book will undoubtedly fall short of the high standard set by that
  famous physicist, perhaps it will be good enough for you in your quest to
  understand what operating systems (and more generally, systems) are all
  about.

'''

* The three easy pieces refer to the three major thematic elements the book is
  organized around: *virtualization*, *concurrency*, and *persistence*.
* In discussing these concepts, we'll end up discussing most of the important
  things an operating system does; hopefully, you'll also have some fun along
  the way.

'''

* Each major concept is divided into a set of chapters, most of which present a
  particular problem and then show how to solve it.
* The chapters are short, and try (as best as possible) to reference the source
  material where the ideas really came from.
* One of our goals in writing this book is to make the paths of history as
  clear as possible, as we think that helps a student understand what is, what
  was, and what will be more clearly.

'''

* There are a couple devices we use throughout the book which are probably
  worth introducing here.
* The first is the *crux* of the problem.
* Anytime we are trying to solve a problem, we first try to state what the most
  important issue is; such a *crux of the problem* is explicitly called out
  in the text, and hopefully solved via the techniques, algorithms, and ideas
  presented in the rest of the text.
* In many places, we'll explain how a system works by showing its behavior over
  time.
* These *timelines* are at the essence of understanding; if you know what
  happens, for example, when a process page faults, you are on your way to
  truly understanding how virtual memory operates.
* If you comprehend what takes place when a journaling file system writes a
  block to disk, you have taken the first steps towards mastery of storage
  systems.

'''

* There are also numerous *asides* and *tips* throughout the text, adding a
  little color to the mainline presentation.
* Asides tend to discuss something relevant (but perhaps not essential) to the
  main text; tips tend to be general lessons that can be applied to systems you
  build.
* An index at the end of the book lists all of these tips and asides (as well
  as cruces, the odd plural of crux) for your convenience.

'''

* We use one of the oldest didactic methods, the *dialogue*, throughout the
  book, as a way of presenting some of the material in a different light.
* These are used to introduce the major thematic concepts (in a peachy way, as
  we will see), as well as to review material every now and then.
* They are also a chance to write in a more humorous style.
* Whether you find them useful, or humorous, well, that's another matter
  entirely.

'''

* At the beginning of each major section, we'll first present an
  *abstraction* that an operating system provides, and then work in subsequent
  chapters on the mechanisms, policies, and other support needed to
  provide the abstraction.
* Abstractions are fundamental to all aspects of Computer Science, so it is
  perhaps no surprise that they are also essential in operating systems.

'''

* Throughout the chapters, we try to use *real code* (not *pseudocode*) where
  possible, so for virtually all examples, you should be able to type
  them up yourself and run them.
* Running real code on real systems is the best way to learn about operating
  systems, so we encourage you to do so when you can.
* We are also making code available for your viewing
  pleasurefootnote:[https://github.com/remzi-arpacidusseau/ostep-code].

'''

* In various parts of the text, we have sprinkled in a few *homeworks* to
  ensure that you are understanding what is going on.
* Many of these homeworks are little *simulations* of pieces of the operating
  system; you should download the homeworks, and run them to quiz yourself.
* The homework simulators have the following feature: by giving them a
  different random seed, you can generate a virtually infinite set of problems;
  the simulators can also be told to solve the problems for you.
* Thus, you can test and re-test yourself until you have achieved a good level
  of understanding.

'''

* The most important addendum to this book is a set of *projects* in which you
  learn about how real systems work by designing, implementing, and testing
  your own code.
* All projects (as well as the code examples, mentioned above) are in the *C
  programming language* [KR88]; C is a simple and powerful language that
  underlies most operating systems, and thus worth adding to your tool-chest of
  languages.
* Two types of projects are available (see the online appendix for ideas).
* The first type is systems programming projects; these projects are great for
  those who are new to C and UNIX and want to learn how to do low-level C
  programming.
* The second type is based on a real operating system kernel developed at MIT
  called xv6 [CK+08]; these projects are great for students that already have
  some C and want to get their hands dirty inside the OS.
* At Wisconsin, we've run the course in three different ways: either all
  systems programming, all xv6 programming, or a mix of both.

'''

* We are slowly making project descriptions, and a testing framework,
  available.
* See our
  repositoryfootnote:[https://github.com/remzi-arpacidusseau/ostep-projects]
  for more information.
* If not part of a class, this will give you a chance to do these projects on
  your own, to better learn the material.
* Unfortunately, you don't have a TA to bug when you get stuck, but not
  everything in life can be free (but books can be!).

=== To Educators

* The course divides fairly well across a 15-week semester, in which you can
  cover most of the topics within at a reasonable level of depth.
* Cramming the course into a 10-week quarter probably requires dropping some
  detail from each of the pieces.
* There are also a few chapters on virtual machine monitors, which we usually
  squeeze in sometime during the semester, either right at end of the large
  section on virtualization, or near the end as an aside.

'''

* One slightly unusual aspect of the book is that concurrency, a topic at the
  front of many OS books, is pushed off herein until the student has built an
  understanding of virtualization of the CPU and of memory.
* In our experience in teaching this course for nearly 20 years, students have
  a hard time understanding how the concurrency problem arises, or why they are
  trying to solve it, if they don't yet understand what an address space is, what
  a process is, or why context switches can occur at arbitrary points in time.
* Once they do understand these concepts, however, introducing the notion of
  threads and the problems that arise due to them becomes rather easy, or at
  least, easier.

'''

* As much as is possible, we use a chalkboard (or whiteboard) to deliver a
  lecture.
* On these more conceptual days, we come to class with a few major ideas and
  examples in mind and use the board to present them.
* Handouts are useful to give the students concrete problems to solve based on
  the material.
* On more practical days, we simply plug a laptop into the projector and show
  real code; this style works particularly well for concurrency lectures as
  well as for any discussion sections where you show students code that is
  relevant for their projects.
* We don't generally use slides to present material, but have now made a set
  available for those who prefer that style of presentation.

'''

* One last request: if you use the free online chapters, please just link to
  them, instead of making a local copy.
* This helps us track usage (million of chapters downloaded each month) and
  also ensures students get the latest (and greatest?) version.

=== To Students

* We both think back fondly towards some textbooks of our undergraduate days
  (e.g., Hennessy and Patterson [HP90], the classic book on computer
  architecture) and hope this book will become one of those positive memories for
  you.

'''

* You may have noticed this book is free and available online.
* There is one major reason for this: textbooks are generally too expensive.
* This book, we hope, is the first of a new wave of free materials to help
  those in pursuit of their education, regardless of which part of the world
  they come from or how much they are willing to spend for a book.
* Failing that, it is one free book, which is better than none.

'''

* We also hope, where possible, to point you to the original sources of much of
  the material in the book: the great papers and persons who have shaped the
  field of operating systems over the years.
* Ideas are not pulled out of the air; they come from smart and hard-working
  people (including numerous Turing-award winners), and thus we should strive
  to celebrate those ideas and people where possible.
* In doing so, we hopefully can better understand the revolutions that have
  taken place, instead of writing texts as if those thoughts have always been
  present [K62].
* Further, perhaps such references will encourage you to dig deeper on your
  own; reading the famous papers of our field is certainly one of the best ways
  to learn.

=== Final Words

* Yeats famously said "Education is not the filling of a pail but the lighting
  of a fire."
* He was right but wrong at the same time.
* You do have to "fill the pail" a bit, and these notes are certainly here to
  help with that part of your education; after all, when you go to interview at
  Google, and they ask you a trick question about how to use semaphores, it might
  be good to actually know what a semaphore is, right?

'''

* But Yeats's larger point is obviously on the mark: the real point of
  education is to get you interested in something, to learn something more
  about the subject matter on your own and not just what you have to digest to
  get a good grade in some class.

'''

* We created these notes to spark your interest in operating systems, to read
  more about the topic on your own, to talk to your professor about all the
  exciting research that is going on in the field, and even to get involved
  with that research.
* It is a great field(!), full of exciting and wonderful ideas that have shaped
  computing history in profound and important ways.
* And while we understand this fire won't light for all of you, we hope it does
  for many, or even a few.
* Because once that fire is lit, well, that is when you truly become capable of
  doing something great.
* And thus the real point of the educational process: to go forth, to study
  many new and fascinating topics, to learn, to mature, and most importantly,
  to find something that lights a fire for you.

=== References

.[CK+08] "The xv6 Operating System" by Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich. From: `http://pdos.csail.mit.edu/6.828/2008/index.html`.
* xv6 was developed as a port of the original UNIX version 6 and represents a
  beautiful, clean, and simple way to understand a modern operating system.

.[F96] "Six Easy Pieces: Essentials Of Physics Explained By Its Most Brilliant Teacher" by Richard P. Feynman. Basic Books, 1996.
* This book reprints the six easiest chapters of Feynman's Lectures on Physics,
  from 1963.
* If you like Physics, it is a fantastic read.

.[HP90] "Computer Architecture a Quantitative Approach" (1st ed.) by David A. Patterson and John L. Hennessy. Morgan-Kaufman, 1990.

* A book that encouraged each of us at our undergraduate institutions to pursue
  graduate studies; we later both had the pleasure of working with Patterson,
  who greatly shaped the foundations of our research careers.

.[KR88] "The C Programming Language" by Brian Kernighan and Dennis Ritchie. PrenticeHall, April 1988.
* The C programming reference that everyone should have, by the people who
  invented the language.

.[K62] "The Structure of Scientific Revolutions" by Thomas S. Kuhn. University of Chicago Press, 1962.
* A great and famous read about the fundamentals of the scientific process.
* Mop-up work, anomaly, crisis, and revolution.
* We are mostly destined to do mop-up work, alas.

== Contents

=== Intro

* <<Preface>>
* <<TOC>>
* [x] link:01-a-dialogue-on-the-book.adoc[1 A Dialogue on the Book] 1

* link:02-introduction-to-operating-systems/[2 Introduction to Operating
  Systems 3]
** [x] 2.1 Virtualizing The CPU 5
** [x] 2.2 virtualizing Memory 7
** [x] 2.3 Concurrency 9 
** [ ] 2.4 Persistence 11
** [ ] 2.5 Design Goals 13
** [ ] 2.6 Some History 14
** [ ] 2.7 Summary 19
** [ ] References 20
** [ ] Homework 21

=== 1 Virtualization

* [ ] 3 A Dialogue on Virtualization 25
* 4 The Abstraction: The Process 27
** [ ] 4.1 The Abstraction: A Process 28
** [ ] 4.2 Process API 29
** [ ] 4.3 Process Creation: A Little More Detail 30
** [ ] 4.4 Process States 31
** [ ] 4.5 Data Structures 33
** [ ] 4.6 Summary 35
** [ ] References 37
** [ ] Homework (Simulation) 38

* 5 Interlude: Process API 41
** [ ] 5.1 The fork() System Call 41
** [ ] 5.2 The wait() System Call 44
** [ ] 5.3 Finally, The exec() System Call 44
** [ ] 5.4 Why? Motivating The API 46
** [ ] 5.5 Process Control And Users 48
** [ ] 5.6 Useful Tools 49
** [ ] 5.7 Summary 50
** [ ] References 52
** [ ] Homework (Simulation) 53
** [ ] Homework (Code) 54

* 6 Mechanism: Limited Direct Execution 57
** [ ] 6.1 Basic Technique: Limited Direct Execution 57
** [ ] 6.2 Problem #1: Restricted Operations 58
** [ ] 6.3 Problem #2: Switching Between Processes 63
** [ ] 6.4 Worried About Concurrency? 67
** [ ] 6.5 Summary 68
** [ ] References 71
** [ ] Homework (Measurement) 72

*  7 Scheduling: Introduction 73
** [ ] 7.1 Workload Assumptions 73
** [ ] 7.2 Scheduling Metrics 74
** [ ] 7.3 First In, First Out (FIFO) 74
** [ ] 7.4 Shortest Job First (SJF) 76
** [ ] 7.5 Shortest Time-to-Completion First (STCF) 77
** [ ] 7.6 A New Metric: Response Time 78
** [ ] 7.7 Round Robin 79
** [ ] 7.8 Incorporating I/O 81
** [ ] 7.9 No More Oracle 82
** [ ] 7.10 Summary 83
** [ ] References 84
** [ ] Homework (Simulation) 85

* 8 Scheduling: The Multi-Level Feedback Queue 87
** [ ] 8.1 MLFQ: Basic Rules 88
** [ ] 8.2 Attempt #1: How To Change Priority 89
** [ ] 8.3 Attempt #2: The Priority Boost 92
** [ ] 8.4 Attempt #3: Better Accounting 93
** [ ] 8.5 Tuning MLFQ And Other Issues 94
** [ ] 8.6 MLFQ: Summary 96
** [ ] References 97
** [ ] Homework (Simulation) 98

* 9 Scheduling: Proportional Share 99
** [ ] 9.1 Basic Concept: Tickets Represent Your Share 99
** [ ] 9.2 Ticket Mechanisms 101
** [ ] 9.3 Implementation 102
** [ ] 9.4 An Example 103
** [ ] 9.5 How To Assign Tickets? 104
** [ ] 9.6 Stride Scheduling 104
** [ ] 9.7 The Linux Completely Fair Scheduler (CFS) 105
** [ ] 9.8 Summary 110
** [ ] References 111
** [ ] Homework (Simulation) 112

* 10 Multiprocessor Scheduling (Advanced) 113
** [ ] 10.1 Background: Multiprocessor Architecture 114
** [ ] 10.2 Don’t Forget Synchronization 116
** [ ] 10.3 One Final Issue: Cache Affinity 117
** [ ] 10.4 Single-Queue Scheduling 118
** [ ] 10.5 Multi-Queue Scheduling 119
** [ ] 10.6 Linux Multiprocessor Schedulers 122
** [ ] 10.7 Summary 122
** [ ] References 123
** [ ] Homework (Simulation) 124

* [ ] 11 Summary Dialogue on CPU Virtualization 127
* [ ] 12 A Dialogue on Memory Virtualization 129

* 13 The Abstraction: Address Spaces 131
** [ ] 13.1 Early Systems 131
** [ ] 13.2 Multiprogramming and Time Sharing 131
** [ ] 13.3 The Address Space 133
** [ ] 13.4 Goals 135
** [ ] 13.5 Summary 136
** [ ] References 138
** [ ] Homework (Code) 139

* 14 Interlude: Memory API 141
** [ ] 14.1 Types of Memory 141
** [ ] 14.2 The malloc() Call 142
** [ ] 14.3 The free() Call 144
** [ ] 14.4 Common Errors 144
** [ ] 14.5 Underlying OS Support 148
** [ ] 14.6 Other Calls 148
** [ ] 14.7 Summary 149
** [ ] References 150
** [ ] Homework (Code) 151

* 15 Mechanism: Address Translation 153
** [ ] 15.1 Assumptions 154
** [ ] 15.2 An Example 154
** [ ] 15.3 Dynamic (Hardware-based) Relocation 157
** [ ] 15.4 Hardware Support: A Summary 160
** [ ] 15.5 Operating System Issues 161
** [ ] 15.6 Summary 163
** [ ] References 166
** [ ] Homework (Simulation) 167

* 16 Segmentation 169
** [ ] 16.1 Segmentation: Generalized Base/Bounds 169
** [ ] 16.2 Which Segment Are We Referring To? 172
** [ ] 16.3 What About The Stack? 174
** [ ] 16.4 Support for Sharing 175
** [ ] 16.5 Fine-grained vsCoarse-grained Segmentation 175
** [ ] 16.6 OS Support 176
** [ ] 16.7 Summary 178
** [ ] References 179
** [ ] Homework (Simulation) 180

* 17 Free-Space Management 181
** [ ] 17.1 Assumptions 182
** [ ] 17.2 Low-level Mechanisms 183
** [ ] 17.3 Basic Strategies 191
** [ ] 17.4 Other Approaches 193
** [ ] 17.5 Summary 195
** [ ] References 197
** [ ] Homework (Simulation) 198

* 18 Paging: Introduction 199
** [ ] 18.1 A Simple Example And Overview 199
** [ ] 18.2 Where Are Page Tables Stored? 203
** [ ] 18.3 What’s Actually In The Page Table? 204
** [ ] 18.4 Paging: Also Too Slow 206
** [ ] 18.5 A Memory Trace 207
** [ ] 18.6 Summary 210
** [ ] References 211
** [ ] Homework (Simulation) 212

* 19 Paging: Faster Translations (TLBs) 215
** [ ] 19.1 TLB Basic Algorithm 216
** [ ] 19.2 Example: Accessing An Array 217
** [ ] 19.3 Who Handles The TLB Miss? 220
** [ ] 19.4 TLB Contents: What’s In There? 222
** [ ] 19.5 TLB Issue: Context Switches 223
** [ ] 19.6 Issue: Replacement Policy 225
** [ ] 19.7 A Real TLB Entry 225
** [ ] 19.8 Summary 226
** [ ] References 228
** [ ] Homework (Measurement) 229

* 20 Paging: Smaller Tables 231
** [ ] 20.1 Simple Solution: Bigger Pages 231
** [ ] 20.2 Hybrid Approach: Paging and Segments 232
** [ ] 20.3 Multi-level Page Tables 235
** [ ] 20.4 Inverted Page Tables 243
** [ ] 20.5 Swapping the Page Tables to Disk 243
** [ ] 20.6 Summary 243
** [ ] References 244
** [ ] Homework (Simulation) 245

* 21 Beyond Physical Memory: Mechanisms 247
** [ ] 21.1 Swap Space 248
** [ ] 21.2 The Present Bit 249
** [ ] 21.3 The Page Fault 250
** [ ] 21.4 What If Memory Is Full? 251
** [ ] 21.5 Page Fault Control Flow 252
** [ ] 21.6 When Replacements Really Occur 253
** [ ] 21.7 Summary 254
** [ ] References 255
** [ ] Homework (Measurement) 256

* 22 Beyond Physical Memory: Policies 259
** [ ] 22.1 Cache Management 259
** [ ] 22.2 The Optimal Replacement Policy 260
** [ ] 22.3 A Simple Policy: FIFO 262
** [ ] 22.4 Another Simple Policy: Random 264
** [ ] 22.5 Using History: LRU 265
** [ ] 22.6 Workload Examples 266
** [ ] 22.7 Implementing Historical Algorithms 269
** [ ] 22.8 Approximating LRU 270
** [ ] 22.9 Considering Dirty Pages 271
** [ ] 22.10 Other VM Policies 272
** [ ] 22.11 Thrashing 272
** [ ] 22.12 Summary 273
** [ ] References 274
** [ ] Homework (Simulation) 276

* 23 Complete Virtual Memory Systems 277
** [ ] 23.1 VAX/VMS Virtual Memory 278
** [ ] 23.2 The Linux Virtual Memory System 284
** [ ] 23.3 Summary 293
** [ ] References 295

* [ ] 24 Summary Dialogue on Memory Virtualization 297

=== 2 Concurrency

* [ ] 25 A Dialogue on Concurrency 303

* 26 Concurrency: An Introduction 305
** [ ] 26.1 Why Use Threads? 306
** [ ] 26.2 An Example: Thread Creation 307
** [ ] 26.3 Why It Gets Worse: Shared Data 310
** [ ] 26.4 The Heart Of The Problem: Uncontrolled Scheduling 313
** [ ] 26.5 The Wish For Atomicity 315
** [ ] 26.6 One More Problem: Waiting For Another 316
** [ ] 26.7 Summary: Why in OS Class? 317
** [ ] References 318
** [ ] Homework (Simulation) 319

* 27 Interlude: Thread API 321
** [ ] 27.1 Thread Creation 321
** [ ] 27.2 Thread Completion 322
** [ ] 27.3 Locks 325
** [ ] 27.4 Condition Variables 327
** [ ] 27.5 Compiling and Running 329
** [ ] 27.6 Summary 329
** [ ] References 331
** [ ] Homework (Code) 332

* 28 Locks 333
** [ ] 28.1 Locks: The Basic Idea 333
** [ ] 28.2 Pthread Locks 334
** [ ] 28.3 Building A Lock 335
** [ ] 28.4 Evaluating Locks 335
** [ ] 28.5 Controlling Interrupts 336
** [ ] 28.6 A Failed Attempt: Just Using Loads/Stores 337
** [ ] 28.7 Building Working Spin Locks with Test-And-Set 338
** [ ] 28.8 Evaluating Spin Locks 341
** [ ] 28.9 Compare-And-Swap 342
** [ ] 28.10 Load-Linked and Store-Conditional 343
** [ ] 28.11 Fetch-And-Add 344
** [ ] 28.12 Too Much Spinning: What Now? 345
** [ ] 28.13 A Simple Approach: Just Yield, Baby 346
** [ ] 28.14 Using Queues: Sleeping Instead Of Spinning 347
** [ ] 28.15 Different OS, Different Support 350
** [ ] 28.16 Two-Phase Locks 352
** [ ] 28.17 Summary 352
** [ ] References 353
** [ ] Homework (Simulation) 354

* 29 Lock-based Concurrent Data Structures 355
** [ ] 29.1 Concurrent Counters 355
** [ ] 29.2 Concurrent Linked Lists 361
** [ ] 29.3 Concurrent Queues 364
** [ ] 29.4 Concurrent Hash Table 366
** [ ] 29.5 Summary 366
** [ ] References 369
** [ ] Homework (Code) 370

* 30 Condition Variables 371
** [ ] 30.1 Definition and Routines 372
** [ ] 30.2 The Producer/Consumer (Bounded Buffer) Problem 376
** [ ] 30.3 Covering Conditions 384
** [ ] 30.4 Summary 386
** [ ] References 387
** [ ] Homework (Code) 388

* 31 Semaphores 391
** [ ] 31.1 Semaphores: A Definition 391
** [ ] 31.2 Binary Semaphores (Locks) 393
** [ ] 31.3 Semaphores For Ordering 394
** [ ] 31.4 The Producer/Consumer (Bounded Buffer) Problem 396
** [ ] 31.5 Reader-Writer Locks 401
** [ ] 31.6 The Dining Philosophers 403
** [ ] 31.7 Thread Throttling 406
** [ ] 31.8 How To Implement Semaphores 406
** [ ] 31.9 Summary 407
** [ ] References 409
** [ ] Homework (Code) 410

* 32 Common Concurrency Problems 411
** [ ] 32.1 What Types Of Bugs Exist? 411
** [ ] 32.2 Non-Deadlock Bugs 412
** [ ] 32.3 Deadlock Bugs 415
** [ ] 32.4 Summary 424
** [ ] References 425
** [ ] Homework (Code) 426

* 33 Event-based Concurrency (Advanced) 427
** [ ] 33.1 The Basic Idea: An Event Loop 427
** [ ] 33.2 An Important API: select() (or poll()) 428
** [ ] 33.3 Using select() 429
** [ ] 33.4 Why Simpler? No Locks Needed 431
** [ ] 33.5 A Problem: Blocking System Calls 431
** [ ] 33.6 A Solution: Asynchronous I/O 432
** [ ] 33.7 Another Problem: State Management 433
** [ ] 33.8 What Is Still Difficult With Events 435
** [ ] 33.9 Summary 436
** [ ] References 437
** [ ] Homework (Code) 438

* [ ] 34 Summary Dialogue on Concurrency 439

=== 3 Persistence

* [ ] 35 A Dialogue on Persistence 443

* 36 I/O Devices 445
** [ ] 36.1 System Architecture 445
** [ ] 36.2 A Canonical Device 447
** [ ] 36.3 The Canonical Protocol 448
** [ ] 36.4 Lowering CPU Overhead With Interrupts 449
** [ ] 36.5 More Efficient Data Movement With DMA 450
** [ ] 36.6 Methods Of Device Interaction 451
** [ ] 36.7 Fitting Into The OS: The Device Driver 452
** [ ] 36.8 Case Study: A Simple IDE Disk Driver 453
** [ ] 36.9 Historical Notes 455
** [ ] 36.10 Summary 457
** [ ] References 458

* 37 Hard Disk Drives 459
** [ ] 37.1 The Interface 459
** [ ] 37.2 Basic Geometry 460
** [ ] 37.3 A Simple Disk Drive 461
** [ ] 37.4 I/O Time: Doing The Math 464
** [ ] 37.5 Disk Scheduling 468
** [ ] 37.6 Summary 472
** [ ] References 473
** [ ] Homework (Simulation) 474

* 38 Redundant Arrays of Inexpensive Disks (RAIDs) 475
** [ ] 38.1 Interface And RAID Internals 476
** [ ] 38.2 Fault Model 477
** [ ] 38.3 How To Evaluate A RAID 477
** [ ] 38.4 RAID Level 0: Striping 478
** [ ] 38.5 RAID Level 1: Mirroring 481
** [ ] 38.6 RAID Level 4: Saving Space With Parity 484
** [ ] 38.7 RAID Level 5: Rotating Parity 488
** [ ] 38.8 RAID Comparison: A Summary 489
** [ ] 38.9 Other Interesting RAID Issues 490
** [ ] 38.10 Summary 490
** [ ] References 491
** [ ] Homework (Simulation) 492

* 39 Interlude: Files and Directories 493
** [ ] 39.1 Files And Directories 493
** [ ] 39.2 The File System Interface 495
** [ ] 39.3 Creating Files 495
** [ ] 39.4 Reading And Writing Files 497
** [ ] 39.5 Reading And Writing, But Not Sequentially 499
** [ ] 39.6 Shared File Table Entries: fork() And dup() 501
** [ ] 39.7 Writing Immediately With fsync() 504
** [ ] 39.8 Renaming Files 504
** [ ] 39.9 Getting Information About Files 506
** [ ] 39.10 Removing Files 507
** [ ] 39.11 Making Directories 508
** [ ] 39.12 Reading Directories 509
** [ ] 39.13 Deleting Directories 510
** [ ] 39.14 Hard Links 510
** [ ] 39.15 Symbolic Links 512
** [ ] 39.16 Permission Bits And Access Control Lists 514
** [ ] 39.17 Making And Mounting A File System 516
** [ ] 39.18 Summary 518
** [ ] References 520
** [ ] Homework (Code) 521

* 40 File System Implementation 523
** [ ] 40.1 The Way To Think 523
** [ ] 40.2 Overall Organization 524
** [ ] 40.3 File Organization: The Inode 526
** [ ] 40.4 Directory Organization 530
** [ ] 40.5 Free Space Management 532
** [ ] 40.6 Access Paths: Reading and Writing 532
** [ ] 40.7 Caching and Buffering 536
** [ ] 40.8 Summary 538
** [ ] References 539
** [ ] Homework (Simulation) 540

* 41 Locality and The Fast File System 541
** [ ] 41.1 The Problem: Poor Performance 541
** [ ] 41.2 FFS: Disk Awareness Is The Solution 543
** [ ] 41.3 Organizing Structure: The Cylinder Group 543
** [ ] 41.4 Policies: How To Allocate Files and Directories 545
** [ ] 41.5 Measuring File Locality 547
** [ ] 41.6 The Large-File Exception 548
** [ ] 41.7 A Few Other Things About FFS 550
** [ ] 41.8 Summary 552
** [ ] References 553
** [ ] Homework (Simulation) 554

* 42 Crash Consistency: FSCK and Journaling 555
** [ ] 42.1 A Detailed Example 556
** [ ] 42.2 Solution #1: The File System Checker 559
** [ ] 42.3 Solution #2: Journaling (or Write-Ahead Logging) 561
** [ ] 42.4 Solution #3: Other Approaches 571
** [ ] 42.5 Summary 572
** [ ] References 573
** [ ] Homework (Simulation) 575

* 43 Log-structured File Systems 577
** [ ] 43.1 Writing To Disk Sequentially 578
** [ ] 43.2 Writing Sequentially And Effectively 579
** [ ] 43.3 How Much To Buffer? 580
** [ ] 43.4 Problem: Finding Inodes 581
** [ ] 43.5 Solution Through Indirection: The Inode Map 581
** [ ] 43.6 Completing The Solution: The Checkpoint Region 583
** [ ] 43.7 Reading A File From Disk: A Recap 583
** [ ] 43.8 What About Directories? 584
** [ ] 43.9 A New Problem: Garbage Collection 585
** [ ] 43.10 Determining Block Liveness 586
** [ ] 43.11 A Policy Question: Which Blocks To Clean, And When? 587
** [ ] 43.12 Crash Recovery And The Log 588
** [ ] 43.13 Summary 588
** [ ] References 590
** [ ] Homework (Simulation) 591

* 44 Flash-based SSDs 593
** [ ] 44.1 Storing a Single Bit 593
** [ ] 44.2 From Bits to Banks/Planes 594
** [ ] 44.3 Basic Flash Operations 595
** [ ] 44.4 Flash Performance And Reliability 597
** [ ] 44.5 From Raw Flash to Flash-Based SSDs 598
** [ ] 44.6 FTL Organization: A Bad Approach 599
** [ ] 44.7 A Log-Structured FTL 600
** [ ] 44.8 Garbage Collection 602
** [ ] 44.9 Mapping Table Size 604
** [ ] 44.10 Wear Leveling 609
** [ ] 44.11 SSD Performance And Cost 609
** [ ] 44.12 Summary 611
** [ ] References 613
** [ ] Homework (Simulation) 615

* 45 Data Integrity and Protection 617
** [ ] 45.1 Disk Failure Modes 617
** [ ] 45.2 Handling Latent Sector Errors 619
** [ ] 45.3 Detecting Corruption: The Checksum 620
** [ ] 45.4 Using Checksums 623
** [ ] 45.5 A New Problem: Misdirected Writes 624
** [ ] 45.6 One Last Problem: Lost Writes 625
** [ ] 45.7 Scrubbing 625
** [ ] 45.8 Overheads Of Checksumming 626
** [ ] 45.9 Summary 627
** [ ] References 628
** [ ] Homework (Simulation) 629
** [ ] Homework (Code) 630

* [ ] 46 Summary Dialogue on Persistence 631
* [ ] 47 A Dialogue on Distribution 633

* 48 Distributed Systems 635
** [ ] 48.1 Communication Basics 636
** [ ] 48.2 Unreliable Communication Layers 637
** [ ] 48.3 Reliable Communication Layers 639
** [ ] 48.4 Communication Abstractions 642
** [ ] 48.5 Remote Procedure Call (RPC) 643
** [ ] 48.6 Summary 648
** [ ] References 649
** [ ] Homework (Code) 650

* 49 Sun's Network File System (NFS) 653
** [ ] 49.1 A Basic Distributed File System 654
** [ ] 49.2 On To NFS 655
** [ ] 49.3 Focus: Simple And Fast Server Crash Recovery 655
** [ ] 49.4 Key To Fast Crash Recovery: Statelessness 656
** [ ] 49.5 The NFSv2 Protocol 657
** [ ] 49.6 From Protocol To Distributed File System 659
** [ ] 49.7 Handling Server Failure With Idempotent Operations 661
** [ ] 49.8 Improving Performance: Client-side Caching 663
** [ ] 49.9 The Cache Consistency Problem 663
** [ ] 49.10 Assessing NFS Cache Consistency 665
** [ ] 49.11 Implications On Server-Side Write Buffering 665
** [ ] 49.12 Summary 667
** [ ] References 669
** [ ] Homework (Measurement) 670

* 50 The Andrew File System (AFS) 671
** [ ] 50.1 AFS Version 1 671
** [ ] 50.2 Problems with Version 1 673
** [ ] 50.3 Improving the Protocol 674
** [ ] 50.4 AFS Version 2 674
** [ ] 50.5 Cache Consistency 676
** [ ] 50.6 Crash Recovery 678
** [ ] 50.7 Scale And Performance Of AFSv2 679
** [ ] 50.8 AFS: Other Improvements 681
** [ ] 50.9 Summary 682
** [ ] References 683
** [ ] Homework (Simulation) 684

* [ ] 51 Summary Dialogue on Distribution 685

=== Security

* [ ] 52 Dialogue
* [ ] 53 Intro Security
* [ ] 54 Authentication
* [ ] 55 Access Control
* [ ] 56 Cryptography
* [ ] 57 Distributed

=== Appendices

* [x] link:appendix-a-a-dialogue-on-virtual-machine-monitors.adoc[A Dialogue]
* link:appendix-b-virtual-machine-monitors.adoc[B Virtual Machines]
** [x] B.1 Introduction
** [x] B.2 Motivation: Why VMMs?
** [ ] B.3 Virtualizing the CPU
** [ ] B.4 Virtualizing Memory
** [ ] B.5 The information Gap
** [ ] B.6 Summary
** [ ] References
* C Dialogue
* D Monitors
* E Dialogue
* F Lba Tutorial
* G Systems Labs
* H xv6 Labs
