= 29 Lock-based Concurrent Data Structures
:figure-caption: Figure 29.
:source-highlighter: rouge
:stem: latexmath
:tabsize: 8
:toc: left

* Before moving beyond locks, we'll first describe how to use locks in some
  common data structures.
* Adding locks to a data structure to make it usable by threads makes the
  structure *thread safe*.
* Of course, exactly how such locks are added determines both the correctness
  and performance of the data structure.
* And thus, our challenge:

.Crux: How to add locks to data structures
****
* When given a particular data structure, how should we add locks to it, in
  order to make it work correctly?
* Further, how do we add locks such that the data structure yields high
  performance, enabling many threads to access the structure at once, i.e.,
  *concurrently*?
****

* Of course, we will be hard pressed to cover all data structures or all
  methods for adding concurrency, as this is a topic that has been studied for
  years, with (literally) thousands of research papers published about it.
* Thus, we hope to provide a sufficient introduction to the type of thinking
  required, and refer you to some good sources of material for further inquiry
  on your own.
* We found Moir and Shavit's survey to be a great source of information
  [MS04].

== 29.1 Concurrent Counters

* One of the simplest data structures is a counter.
* It is a structure that is commonly used and has a simple interface.
* We define a simple non concurrent counter in Figure 29.1.

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. A Counter Without Locks
[,c]
----
typedef struct __counter_t {
	int value;
} counter_t;

void init(counter_t *c) {
	c->value = 0;
}

void increment(counter_t *c) {
	c->value++;
}

void decrement(counter_t *c) {
	c->value--;
}

int get(counter_t *c) {
	return c->value;
}
----

=== Simple But Not Scalable

* As you can see, the non-synchronized counter is a trivial data structure,
  requiring a tiny amount of code to implement.
* We now have our next challenge: how can we make this code *thread safe*?
* Figure 29.2 shows how we do so.

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. A Counter With Locks
[,c]
----
typedef struct __counter_t {
	int value;
	pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
	c->value = 0;
	Pthread_mutex_init(&c->lock, NULL);
}

void increment(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value++;
	Pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value--;
	Pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	int rc = c->value;
	Pthread_mutex_unlock(&c->lock);
	return rc;
}
----

* This concurrent counter is simple and works correctly.
* In fact, it follows a design pattern common to the simplest and most basic
  concurrent data structures: it simply adds a single lock, which is acquired
  when calling a routine that manipulates the data structure, and is
  released when returning from the call.
* In this manner, it is similar to a data structure built with *monitors*
  [BH73], where locks are acquired and released automatically as you call and
  return from object methods.

'''

* At this point, you have a working concurrent data structure.
* The problem you might have is performance.
* If your data structure is too slow, you'll have to do more than just add a
  single lock; such optimizations, if needed, are thus the topic of the rest
  of the chapter.
* Note that if the data structure is _not_ too slow, you are done!
* No need to do something fancy if something simple will work.
* To understand the performance costs of the simple approach, we run a
  benchmark in which each thread updates a single shared counter a fixed
  number of times; we then vary the number of threads.
* Figure 29.5 shows the total time taken, with one to four threads active;
  each thread updates the counter one million times.
* This experiment was run upon an iMac with four Intel 2.7 GHz i5 CPUs; with
  more CPUs active, we hope to get more total work done per unit time.

'''

* From the top line in the figure (labeled 'Precise'), you can see that the
  performance of the synchronized counter scales poorly.
* Whereas a single thread can complete the million counter updates in a tiny
  amount of time (roughly 0.03 seconds), having two threads each update the
  counter one million times concurrently leads to a massive slowdown (taking
  over 5 seconds!).
* It only gets worse with more threads.

'''

* Ideally, you'd like to see the threads complete just as quickly on multiple
  processors as the single thread does on one.
* Achieving this end is called *perfect scaling*; even though more work is
  done, it is done in parallel, and hence the time taken to complete the task
  is not increased.

=== Scalable Counting

* Amazingly, researchers have studied how to build more scalable counters for
  years [MS04].
* Even more amazing is the fact that scalable counters matter, as recent work
  in operating system performance analysis has shown [B+10]; without scalable
  counting, some workloads running on Linux suffer from serious scalability
  problems on multicore machines.

'''

* Many techniques have been developed to attack this problem.
* We'll describe one approach known as an *approximate counter* [C06].

'''

* The approximate counter works by representing a single logical counter via
  numerous local physical counters, one per CPU core, as well as a single
  global counter.
* Specifically, on a machine with four CPUs, there are four local counters and
  one global one.
* In addition to these counters, there are also locks: one for each local
  counter{empty}footnote:[We need the local locks because we assume there may
  be more than one thread on each core. If, instead, only one thread ran on
  each core, no local lock would be needed.], and one for the global counter.

'''

* The basic idea of approximate counting is as follows.
* When a thread running on a given core wishes to increment the counter, it
  increments its local counter; access to this local counter is synchronized
  via the corresponding local lock.
* Because each CPU has its own local counter, threads across CPUs can update
  local counters without contention, and thus updates to the counter are
  scalable.

'''

* However, to keep the global counter up to date (in case a thread wishes
  counter, by acquiring the global lock and incrementing it by the local
  counter's value; the local counter is then reset to zero.
* How often this local-to-global transfer occurs is determined by a threshold
  stem:[S].
* The smaller stem:[S] is, the more the counter behaves like the non-scalable
  counter above; the bigger stem:[S] is, the more scalable the counter, but
  the further off the global value might be from the actual count.
* One could simply acquire all the local locks and the global lock (in a
  specified order, to avoid deadlock) to get an exact value, but that is not
  scalable.

'''

* To make this clear, let's look at an example (Figure 29.3).
* In this example, the threshold stem:[S] is set to 5, and there are threads on each
  of four CPUs updating their local counters stem:[L_1 \dots L_4].
* The global counter value (stem:[G]) is also shown in the trace, with time
  increasing downward.
* At each time step, a local counter may be incremented; if the local value
  reaches the threshold stem:[S], the local value is transferred to the global
  counter and the local counter is reset.

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. Tracing the Approximate Counters
|===
|Time
|stem:[L_1]	|stem:[L_2]	|stem:[L_3]	|stem:[L_4]
|stem:[G]

|0
|0		|0		|0		|0
|0

|1
|0		|0		|1		|1
|0

|2
|1		|0		|2		|1
|0

|3
|2		|0		|3		|1
|0

|4
|3		|0		|3		|2
|0

|5
|4		|1		|3		|3
|0

|6
|5 -> 0		|1		|3		|4
|5 (from stem:[L_1])

|7
|0		|2		|4		|5 -> 0
|10 (from stem:[L_4])
|===

* The lower line in Figure 29.5 (labeled 'Approximate', on page 6) shows the
  performance of approximate counters with a threshold stem:[S] of 1024.
* Performance is excellent; the time taken to update the counter four million
  times on four processors is hardly higher than the time taken to update it
  one million times on one processor.

'''

* Figure 29.6 shows the importance of the threshold value stem:[S], with four
  threads each incrementing the counter 1 million times on four CPUs.
* If stem:[S] is low, performance is poor (but the global count is always
  quite accurate); if stem:[S] is high, performance is excellent, but the
  global count lags (by at most the number of CPUs multiplied by stem:[S]).
* This accuracy/performance trade-off is what approximate counters enable.

'''

* A rough version of an approximate counter is found in Figure 29.4 (page 5).
* Read it, or better yet, run it yourself in some experiments to better
  understand how it works.

.Tip: More concurrency isn't necessarily faster
****
* If the scheme you design adds a lot of overhead (for example, by acquiring
  and releasing locks frequently, instead of once), the fact that it is more
  concurrent may not be important.
* Simple schemes tend to work well, especially if they use costly routines
  rarely.
* Adding more locks and complexity can be your downfall.
* All of that said, there is one way to really know: build both alternatives
  (simple but less concurrent, and complex but more concurrent) and measure
  how they do.
* In the end, you can't cheat on performance; your idea is either faster, or
  it isn't.
****

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. Approximate Counter Implementation
[,c]
----
typedef struct __counter_t {
	int global;			// global count
	pthread_mutex_t glock;		// global lock
	int local[NUMCPUS];		// per-CPU count
	pthread_mutex_t llock[NUMCPUS];	// ... and locks
	int threshold;			// update freq
} counter_t;

// init: record threshold, init locks, init values
// of all local counts and global count
void init(counter_t *c, int threshold) {
	c->threshold = threshold;
	c->global = 0;
	pthread_mutex_init(&c->glock, NULL);
	int i;
	for (i = 0; i < NUMCPUS; i++) {
		c->local[i] = 0;
		pthread_mutex_init(&c->llock[i], NULL);
	}
}

// update: usually, just grab local lock and update
// local amount; once it has risen 'threshold',
// grab global lock and transfer local values to it
void update(counter_t *c, int threadID, int amt) {
	int cpu = threadID % NUMCPUS;
	pthread_mutex_lock(&c->llock[cpu]);
	c->local[cpu] += amt;
	if (c->local[cpu] >= c->threshold) {
		// transfer to global (assumes amt>0)
		pthread_mutex_lock(&c->glock);
		c->global += c->local[cpu];
		pthread_mutex_unlock(&c->glock);
		c->local[cpu] = 0;
	}
	pthread_mutex_unlock(&c->llock[cpu]);
}

// get: just return global amount (approximate)
int get(counter_t *c) {
	pthread_mutex_lock(&c->glock);
	int val = c->global;
	pthread_mutex_unlock(&c->glock);
	return val; // only approximate!
}
----

.Performance of Traditional vs. Approximate Counters
image::figure-29-05.jpg[]

.Scaling Approximate Counters
image::figure-29-06.jpg[]

== 29.2 Concurrent Linked Lists

* We next examine a more complicated structure, the linked list.
* Let's start with a basic approach once again.
* For simplicity, we'll omit some of the obvious routines that such a list
  would have and just focus on concurrent insert and lookup; we'll leave it to
  the reader to think about delete, etc.
* Figure 29.7 shows the code for this rudimentary data structure.

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. Concurrent Linked List
[,c]
----
// basic node structure
typedef struct __node_t {
	int key;
	struct __node_t *next;
} node_t;

// basic list structure (one used per list)
typedef struct __list_t {
	node_t *head;
	pthread_mutex_t lock;
} list_t;

void List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

int List_Insert(list_t *L, int key) {
	pthread_mutex_lock(&L->lock);
	node_t *new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");
		pthread_mutex_unlock(&L->lock);
		return -1; // fail
	}
	new->key = key;
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
	return 0; // success
}

int List_Lookup(list_t *L, int key) {
	pthread_mutex_lock(&L->lock);
	node_t *curr = L->head;
	while (curr) {
		if (curr->key == key) {
			pthread_mutex_unlock(&L->lock);
			return 0; // success
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return -1; // failure
}
----

* As you can see in the code, the code simply acquires a lock in the insert
  routine upon entry, and releases it upon exit.
* One small tricky issue arises if `malloc()` happens to fail (a rare case);
  in this case, the code must also release the lock before failing the insert.

'''

* This kind of exceptional control flow has been shown to be quite error
  prone; a recent study of Linux kernel patches found that a huge fraction of
  bugs (nearly 40%) are found on such rarely-taken code paths (indeed, this
  observation sparked some of our own research, in which we removed all
  memory-failing paths from a Linux file system, resulting in a more robust
  system [S+11]).

'''

* Thus, a challenge: can we rewrite the insert and lookup routines to remain
  correct under concurrent insert but avoid the case where the failure path
  also requires us to add the call to unlock?

'''

* The answer, in this case, is yes.
* Specifically, we can rearrange the code a bit so that the lock and release
  only surround the actual critical section in the insert code, and that a
  common exit path is used in the lookup code.
* The former works because part of the insert actually need not be locked;
  assuming that `malloc()` itself is thread-safe, each thread can call into it
  without worry of race conditions or other concurrency bugs.
* Only when updating the shared list does a lock need to be held.
* See Figure 29.8 for the details of these modifications.

:figure-number: {counter:figure-number}
.{figure-caption} {figure-number}. Concurrent Linked List: Rewritten
[,c]
----
void List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

int List_Insert(list_t *L, int key) {
	// synchronization not needed
	node_t *new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");
		return -1;
	}
	new->key = key;
	// just lock critical section
	pthread_mutex_lock(&L->lock);
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
	return 0; // success
}

int List_Lookup(list_t *L, int key) {
	int rv = -1;
	pthread_mutex_lock(&L->lock);
	node_t *curr = L->head;
	while (curr) {
		if (curr->key == key) {
			rv = 0;
			break;
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return rv; // now both success and failure
}
----

* As for the lookup routine, it is a simple code transformation to jump out of
  the main search loop to a single return path.
* Doing so again reduces the number of lock acquire/release points in the
  code, and thus decreases the chances of accidentally introducing bugs (such
  as forgetting to unlock before returning) into the code.

=== Scaling Linked Lists

* Though we again have a basic concurrent linked list, once again we are in a
  situation where it does not scale particularly well.
* One technique that researchers have explored to enable more concurrency
  within a list is something called *hand-over-hand locking (a.k.a. *lock
  coupling*) [MS04].

'''

* The idea is pretty simple.
* Instead of having a single lock for the entire list, you instead add a lock
  per node of the list.
* When traversing the list, the code first grabs the next node's lock and then
  releases the current node's lock (which inspires the name hand-over-hand).

.Tip: Be wary of locks and control flow
****
* A general design tip, which is useful in concurrent code as well as
  elsewhere, is to be wary of control flow changes that lead to function
  returns, exits, or other similar error conditions that halt the execution of
  a function.
* Because many functions will begin by acquiring a lock, allocating some
  memory, or doing other similar stateful operations, when errors arise, the
  code has to undo all of the state before returning, which is error-prone.
* Thus, it is best to structure code to minimize this pattern.
****

* Conceptually, a hand-over-hand linked list makes some sense; it enables a
  high degree of concurrency in list operations.
* However, in practice, it is hard to make such a structure faster than the
  simple single lock approach, as the overheads of acquiring and releasing
  locks for each node of a list traversal is prohibitive.
* Even with very large lists, and a large number of threads, the concurrency
  enabled by allowing multiple on-going traversals is unlikely to be faster
  than simply grabbing a single lock, performing an operation, and releasing
  it.
* Perhaps some kind of hybrid (where you grab a new lock every so many nodes)
  would be worth investigating.

== References

[B+10] "An Analysis of Linux Scalability to Many Cores_ by Silas Boyd-Wickizer, Austin T.  Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, Nickolai Zeldovich . OSDI '10, Vancouver, Canada, October 2010.::
* A great study of how Linux performs on multicore machines, as well as some
  simple solutions.
* Includes a *neat sloppy* counter to solve one form of the scalable counting
  problem.

[BH73] "Operating System Principles" by Per Brinch Hansen. Prentice-Hall, 1973. Available: `http://portal.acm.org/citation.cfm?id=540365`.::
* One of the first books on operating systems; certainly ahead of its time.
* Introduced monitors as a concurrency primitive.

[C06] "The Search For Fast, Scalable Counters" by Jonathan Corbet. February 1, 2006. Available: `https://lwn.net/Articles/170003`.::
* LWN has many wonderful articles about the latest in Linux.
* This article is a short description of scalable approximate counting; read
  it, and others, to learn more about the latest in Linux.

[MS04] "Concurrent Data Structures" by Mark Moir and Nir Shavit. In Handbook of Data Structures and Applications (Editors D. Metha and S.Sahni). Chapman and Hall/CRC Press, 2004. Available: `www.ostep.org/Citations/concurrent.pdf`.::
* A short but relatively comprehensive reference on concurrent data
  structures.
* Though it is missing some of the latest works in the area (due to its age),
  it remains an incredibly useful reference.

[S+11] "Making the Common Case the Only Case with Anticipatory Memory Allocation" by Swaminathan Sundararaman, Yupu Zhang, Sriram Subramanian, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau . FAST '11, San Jose, CA, February 2011.::
* Our work on removing possibly-failing allocation calls from kernel code
  paths.
* By allocating all potentially needed memory before doing any work, we avoid
  failure deep down in the storage stack.
